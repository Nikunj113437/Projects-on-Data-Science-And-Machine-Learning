{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56b2d30e",
   "metadata": {},
   "source": [
    "### Project Twitter Sentiment Analysis\n",
    "#### Here in this Project, we take Dataset of tweets which are floated on twitter and identifying which of the tweets are Positive, Negative or Neutral\n",
    "#### Take two different Datasets, one is Training Dataset which consists of Tweets along with Airline Sentiment and other one is Testing Dataset which consists of Tweets not Airline Sentiment, and by applying NLP(Natural Language Processing) ,i.e NLTK And Sklearn Classifier, we Predict Airline Sentiment of Testing Dataset.\n",
    "#### Here in this Project, we use three Sklearn Classifier --> SVM(Support Vector Machine), Random Forest Classifier, Multinomial Naive Bayes Classifier, and Decision Tree Classifier and generate three Different Predictions corresponding to each Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fcbf3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53687791",
   "metadata": {},
   "source": [
    "### Preparing Training Dataset\n",
    "#### Importing Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b5df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=pd.read_csv(\"Downloads/training_twitter_x_y_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c6145f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10980, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567900433542488064</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ColeyGirouard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir I am scheduled for the morning, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 20:16:29 -0800</td>\n",
       "      <td>Washington D.C.</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569989168903819264</td>\n",
       "      <td>positive</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WalterFaddoul</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir seeing your workers time in and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 14:36:22 -0800</td>\n",
       "      <td>Indianapolis, Indiana; USA</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>568089179520954368</td>\n",
       "      <td>positive</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LocalKyle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united Flew ORD to Miami and back and  had gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 08:46:29 -0800</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>568928195581513728</td>\n",
       "      <td>negative</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>amccarthy19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@SouthwestAir @dultch97 that's horse radish üò§üê¥</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-20 16:20:26 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Atlantic Time (Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>568594180014014464</td>\n",
       "      <td>negative</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>J_Okayy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united so our flight into ORD was delayed bec...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-19 18:13:11 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment    airline airline_sentiment_gold  \\\n",
       "0  567900433542488064          negative  Southwest                    NaN   \n",
       "1  569989168903819264          positive  Southwest                    NaN   \n",
       "2  568089179520954368          positive     United                    NaN   \n",
       "3  568928195581513728          negative  Southwest                    NaN   \n",
       "4  568594180014014464          negative     United                    NaN   \n",
       "\n",
       "            name negativereason_gold  retweet_count  \\\n",
       "0  ColeyGirouard                 NaN              0   \n",
       "1  WalterFaddoul                 NaN              0   \n",
       "2      LocalKyle                 NaN              0   \n",
       "3    amccarthy19                 NaN              0   \n",
       "4        J_Okayy                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @SouthwestAir I am scheduled for the morning, ...         NaN   \n",
       "1  @SouthwestAir seeing your workers time in and ...         NaN   \n",
       "2  @united Flew ORD to Miami and back and  had gr...         NaN   \n",
       "3     @SouthwestAir @dultch97 that's horse radish üò§üê¥         NaN   \n",
       "4  @united so our flight into ORD was delayed bec...         NaN   \n",
       "\n",
       "               tweet_created              tweet_location  \\\n",
       "0  2015-02-17 20:16:29 -0800             Washington D.C.   \n",
       "1  2015-02-23 14:36:22 -0800  Indianapolis, Indiana; USA   \n",
       "2  2015-02-18 08:46:29 -0800                    Illinois   \n",
       "3  2015-02-20 16:20:26 -0800                         NaN   \n",
       "4  2015-02-19 18:13:11 -0800                         NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0      Atlantic Time (Canada)  \n",
       "1  Central Time (US & Canada)  \n",
       "2  Central Time (US & Canada)  \n",
       "3      Atlantic Time (Canada)  \n",
       "4  Eastern Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44ab44c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@SouthwestAir I am scheduled for the morning, ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@SouthwestAir seeing your workers time in and ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@united Flew ORD to Miami and back and  had gr...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@SouthwestAir @dultch97 that's horse radish üò§üê¥</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@united so our flight into ORD was delayed bec...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0  @SouthwestAir I am scheduled for the morning, ...          negative\n",
       "1  @SouthwestAir seeing your workers time in and ...          positive\n",
       "2  @united Flew ORD to Miami and back and  had gr...          positive\n",
       "3     @SouthwestAir @dultch97 that's horse radish üò§üê¥          negative\n",
       "4  @united so our flight into ORD was delayed bec...          negative"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=df_train[['text','airline_sentiment']]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1aeca87",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents=df_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce7f217",
   "metadata": {},
   "source": [
    "### Spliiting the Tweet text into words using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ebc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweets_train=[]\n",
    "for i in range(len(training_documents)):\n",
    "    tweets_train.append([word_tokenize(training_documents[i][0]),training_documents[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b33cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['@',\n",
       "   'united',\n",
       "   'when',\n",
       "   'I',\n",
       "   'click',\n",
       "   'that',\n",
       "   'link',\n",
       "   'it',\n",
       "   'wants',\n",
       "   'my',\n",
       "   'flight',\n",
       "   'info',\n",
       "   '.',\n",
       "   'I',\n",
       "   'have',\n",
       "   \"n't\",\n",
       "   'booked',\n",
       "   'a',\n",
       "   'flight',\n",
       "   'yet',\n",
       "   '.',\n",
       "   'I',\n",
       "   'read',\n",
       "   'you',\n",
       "   'waive',\n",
       "   'fees',\n",
       "   'for',\n",
       "   'military',\n",
       "   ',',\n",
       "   'it',\n",
       "   'this',\n",
       "   'true',\n",
       "   '?'],\n",
       "  'neutral'],\n",
       " [['@',\n",
       "   'USAirways',\n",
       "   'waited',\n",
       "   'for',\n",
       "   '3',\n",
       "   'hours',\n",
       "   'NO',\n",
       "   'LUGGAGE',\n",
       "   'line',\n",
       "   'too',\n",
       "   'long',\n",
       "   'left',\n",
       "   'airport',\n",
       "   'when',\n",
       "   'flight',\n",
       "   'Cancelled',\n",
       "   'Flighted',\n",
       "   'WHERE',\n",
       "   'does',\n",
       "   'luggage',\n",
       "   'GO',\n",
       "   '?',\n",
       "   'On',\n",
       "   'hold',\n",
       "   'for',\n",
       "   '1',\n",
       "   'hour',\n",
       "   'so',\n",
       "   'far'],\n",
       "  'negative'],\n",
       " [['@',\n",
       "   'JetBlue',\n",
       "   'Finally',\n",
       "   'starting',\n",
       "   'to',\n",
       "   'come',\n",
       "   'out',\n",
       "   '.',\n",
       "   '40',\n",
       "   'minutes',\n",
       "   'and',\n",
       "   'no',\n",
       "   'communication',\n",
       "   'is',\n",
       "   'not',\n",
       "   'acceptable',\n",
       "   '.',\n",
       "   'Letting',\n",
       "   'me',\n",
       "   'down',\n",
       "   'today',\n",
       "   '.'],\n",
       "  'negative']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle(tweets_train)\n",
    "tweets_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd36e2a",
   "metadata": {},
   "source": [
    "### Part Of Speech using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1461532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb189a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06f79d2",
   "metadata": {},
   "source": [
    "### Cleaning the Words using WordNetLemmatizer available in NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcc693f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'!',\n",
       "  '\"',\n",
       "  '#',\n",
       "  '$',\n",
       "  '%',\n",
       "  '&',\n",
       "  \"'\",\n",
       "  '(',\n",
       "  ')',\n",
       "  '*',\n",
       "  '+',\n",
       "  ',',\n",
       "  '-',\n",
       "  '.',\n",
       "  '/',\n",
       "  ':',\n",
       "  ';',\n",
       "  '<',\n",
       "  '=',\n",
       "  '>',\n",
       "  '?',\n",
       "  '@',\n",
       "  '[',\n",
       "  '\\\\',\n",
       "  ']',\n",
       "  '^',\n",
       "  '_',\n",
       "  '`',\n",
       "  'a',\n",
       "  'about',\n",
       "  'above',\n",
       "  'after',\n",
       "  'again',\n",
       "  'against',\n",
       "  'ain',\n",
       "  'all',\n",
       "  'am',\n",
       "  'an',\n",
       "  'and',\n",
       "  'any',\n",
       "  'are',\n",
       "  'aren',\n",
       "  \"aren't\",\n",
       "  'as',\n",
       "  'at',\n",
       "  'be',\n",
       "  'because',\n",
       "  'been',\n",
       "  'before',\n",
       "  'being',\n",
       "  'below',\n",
       "  'between',\n",
       "  'both',\n",
       "  'but',\n",
       "  'by',\n",
       "  'can',\n",
       "  'couldn',\n",
       "  \"couldn't\",\n",
       "  'd',\n",
       "  'did',\n",
       "  'didn',\n",
       "  \"didn't\",\n",
       "  'do',\n",
       "  'does',\n",
       "  'doesn',\n",
       "  \"doesn't\",\n",
       "  'doing',\n",
       "  'don',\n",
       "  \"don't\",\n",
       "  'down',\n",
       "  'during',\n",
       "  'each',\n",
       "  'few',\n",
       "  'for',\n",
       "  'from',\n",
       "  'further',\n",
       "  'had',\n",
       "  'hadn',\n",
       "  \"hadn't\",\n",
       "  'has',\n",
       "  'hasn',\n",
       "  \"hasn't\",\n",
       "  'have',\n",
       "  'haven',\n",
       "  \"haven't\",\n",
       "  'having',\n",
       "  'he',\n",
       "  'her',\n",
       "  'here',\n",
       "  'hers',\n",
       "  'herself',\n",
       "  'him',\n",
       "  'himself',\n",
       "  'his',\n",
       "  'how',\n",
       "  'i',\n",
       "  'if',\n",
       "  'in',\n",
       "  'into',\n",
       "  'is',\n",
       "  'isn',\n",
       "  \"isn't\",\n",
       "  'it',\n",
       "  \"it's\",\n",
       "  'its',\n",
       "  'itself',\n",
       "  'just',\n",
       "  'll',\n",
       "  'm',\n",
       "  'ma',\n",
       "  'me',\n",
       "  'mightn',\n",
       "  \"mightn't\",\n",
       "  'more',\n",
       "  'most',\n",
       "  'mustn',\n",
       "  \"mustn't\",\n",
       "  'my',\n",
       "  'myself',\n",
       "  'needn',\n",
       "  \"needn't\",\n",
       "  'no',\n",
       "  'nor',\n",
       "  'not',\n",
       "  'now',\n",
       "  'o',\n",
       "  'of',\n",
       "  'off',\n",
       "  'on',\n",
       "  'once',\n",
       "  'only',\n",
       "  'or',\n",
       "  'other',\n",
       "  'our',\n",
       "  'ours',\n",
       "  'ourselves',\n",
       "  'out',\n",
       "  'over',\n",
       "  'own',\n",
       "  're',\n",
       "  's',\n",
       "  'same',\n",
       "  'shan',\n",
       "  \"shan't\",\n",
       "  'she',\n",
       "  \"she's\",\n",
       "  'should',\n",
       "  \"should've\",\n",
       "  'shouldn',\n",
       "  \"shouldn't\",\n",
       "  'so',\n",
       "  'some',\n",
       "  'such',\n",
       "  't',\n",
       "  'than',\n",
       "  'that',\n",
       "  \"that'll\",\n",
       "  'the',\n",
       "  'their',\n",
       "  'theirs',\n",
       "  'them',\n",
       "  'themselves',\n",
       "  'then',\n",
       "  'there',\n",
       "  'these',\n",
       "  'they',\n",
       "  'this',\n",
       "  'those',\n",
       "  'through',\n",
       "  'to',\n",
       "  'too',\n",
       "  'under',\n",
       "  'until',\n",
       "  'up',\n",
       "  've',\n",
       "  'very',\n",
       "  'was',\n",
       "  'wasn',\n",
       "  \"wasn't\",\n",
       "  'we',\n",
       "  'were',\n",
       "  'weren',\n",
       "  \"weren't\",\n",
       "  'what',\n",
       "  'when',\n",
       "  'where',\n",
       "  'which',\n",
       "  'while',\n",
       "  'who',\n",
       "  'whom',\n",
       "  'why',\n",
       "  'will',\n",
       "  'with',\n",
       "  'won',\n",
       "  \"won't\",\n",
       "  'wouldn',\n",
       "  \"wouldn't\",\n",
       "  'y',\n",
       "  'you',\n",
       "  \"you'd\",\n",
       "  \"you'll\",\n",
       "  \"you're\",\n",
       "  \"you've\",\n",
       "  'your',\n",
       "  'yours',\n",
       "  'yourself',\n",
       "  'yourselves',\n",
       "  '{',\n",
       "  '|',\n",
       "  '}',\n",
       "  '~'},\n",
       " '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "stops=set(stopwords.words(\"english\"))\n",
    "punctuations=list(string.punctuation)\n",
    "stops.update(punctuations)\n",
    "stops, string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0817053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71325af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(words):\n",
    "    output_words=[]\n",
    "    for w in words:\n",
    "        if w.isalpha():\n",
    "            if w.lower() not in stops:\n",
    "                pos=pos_tag([w])\n",
    "                clean_word=lemmatizer.lemmatize(w,pos=get_simple_pos(pos[0][1]))\n",
    "                output_words.append(clean_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e94326b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tweets_train)):\n",
    "    tweets_train[i]=(clean_tweets(tweets_train[i][0]),tweets_train[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dc3dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=[]\n",
    "tweets=[]\n",
    "for tweet,sentiment in tweets_train:\n",
    "    tweets.append(\" \".join(tweet))\n",
    "    y_train.append(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd06313",
   "metadata": {},
   "source": [
    "### Using Count Vectorizer to get the X Train Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4b278f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vec=CountVectorizer(max_features=2000)\n",
    "x_train_features=count_vec.fit_transform(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1e632",
   "metadata": {},
   "source": [
    "### Preparing Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22d241b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv(\"Downloads/test_twitter_x_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18b3c496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>569682010270101504</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zsalim03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir In car gng to DFW. Pulled over 1h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 18:15:50 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569608307184242688</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sa_craig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir after all, the plane didn‚Äôt land ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 13:22:57 -0800</td>\n",
       "      <td>College Station, TX</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567879304593408001</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DanaChristos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>@SouthwestAir can't believe how many paying cu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 18:52:31 -0800</td>\n",
       "      <td>CT</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569757651539660801</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>rossj987</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways I can legitimately say that I would...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 23:16:24 -0800</td>\n",
       "      <td>Washington, D.C.</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>569900705852608513</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>tranpham18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir still no response from AA. great ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 08:44:51 -0800</td>\n",
       "      <td>New York City</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id     airline airline_sentiment_gold          name  \\\n",
       "0  569682010270101504    American                    NaN      zsalim03   \n",
       "1  569608307184242688    American                    NaN      sa_craig   \n",
       "2  567879304593408001   Southwest                    NaN  DanaChristos   \n",
       "3  569757651539660801  US Airways                    NaN      rossj987   \n",
       "4  569900705852608513    American                    NaN    tranpham18   \n",
       "\n",
       "  negativereason_gold  retweet_count  \\\n",
       "0                 NaN              0   \n",
       "1                 NaN              0   \n",
       "2                 NaN              1   \n",
       "3                 NaN              0   \n",
       "4                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @AmericanAir In car gng to DFW. Pulled over 1h...         NaN   \n",
       "1  @AmericanAir after all, the plane didn‚Äôt land ...         NaN   \n",
       "2  @SouthwestAir can't believe how many paying cu...         NaN   \n",
       "3  @USAirways I can legitimately say that I would...         NaN   \n",
       "4  @AmericanAir still no response from AA. great ...         NaN   \n",
       "\n",
       "               tweet_created       tweet_location               user_timezone  \n",
       "0  2015-02-22 18:15:50 -0800                Texas  Central Time (US & Canada)  \n",
       "1  2015-02-22 13:22:57 -0800  College Station, TX  Central Time (US & Canada)  \n",
       "2  2015-02-17 18:52:31 -0800                   CT  Eastern Time (US & Canada)  \n",
       "3  2015-02-22 23:16:24 -0800     Washington, D.C.  Eastern Time (US & Canada)  \n",
       "4  2015-02-23 08:44:51 -0800        New York City  Eastern Time (US & Canada)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50b7bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_documents=np.array(df_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7b09ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_test=[]\n",
    "for t in testing_documents:\n",
    "    t=clean_tweets(word_tokenize(t))\n",
    "    tweets_test.append(\" \".join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa07bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_features=count_vec.transform(tweets_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1ae6a",
   "metadata": {},
   "source": [
    "### Splitting Training Dataset in order to find Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4b25eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_tweets,x_test_tweets,y_train_tweets,y_test_tweets=train_test_split(x_train_features,y_train,train_size=0.8,\n",
    "                                                                           test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba337a08",
   "metadata": {},
   "source": [
    "### Performing SkLearn Classifier\n",
    "### SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35bcc8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76775956284153"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc=SVC()\n",
    "svc.fit(x_train_tweets,y_train_tweets)\n",
    "svc.score(x_test_tweets,y_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07209267",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc.fit(x_train_features,y_train)\n",
    "y_pred_svc=svc.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87eec428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_pred_svc)\n",
    "df.to_csv('predictions_tweets_svm.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00516e9",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ae552da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7577413479052824"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier()\n",
    "rfc.fit(x_train_tweets,y_train_tweets)\n",
    "rfc.score(x_test_tweets,y_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8e41be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(x_train_features,y_train)\n",
    "y_pred_rfc=rfc.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473d861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_pred_rfc)\n",
    "df.to_csv('predictions_tweets_rfc.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3def564c",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3eeae461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622950819672131"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnv=MultinomialNB(alpha=1)\n",
    "mnv.fit(x_train_tweets,y_train_tweets)\n",
    "mnv.score(x_test_tweets,y_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451559c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnv.fit(x_train_features,y_train)\n",
    "y_pred_mnv=mnv.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b9ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_pred_mnv)\n",
    "df.to_csv('Predictions_tweets_mnv.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a552d7f",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78e29a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6876138433515483"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn import tree\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(x_train_tweets,y_train_tweets)\n",
    "dt.score(x_test_tweets,y_test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a8f5992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.fit(x_train_features,y_train)\n",
    "y_pred_dt=dt.predict(x_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96652627",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(y_pred_dt)\n",
    "df.to_csv('Predictions_tweets_dt.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a882a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
